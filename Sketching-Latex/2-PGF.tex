\section{Probability-Generating Function (PGF)}
For a d-dimensional count vector $\mathbf{X}=(X_1,\dots,X_d)$, the PGF $G_{\mathbf{X}}(\mathbf{t})$ is defined as the expectation of a product of powers of the components of $\mathbf{t}=(t_1,\dots,t_d)$:

$$G_{\mathbf{X}}(\mathbf{t}) = \mathbb{E}\left[\prod_{i=1}^{d} t_i^{X_i}\right] = \mathbb{E}\left[e^{\left\langle \mathbf{X}, \ln(\mathbf{t}) \right\rangle}\right]$$

Considering the expectancy estimation $\hat{\mathbb{E}}[\mathbf{X}] = \frac{1}{n} \sum_{j=1}^{n} \mathbf{X}_j$, where $j$ are the samples of $\mathbf{X}$, it is possible to estimate the PGF from a dataset:

$$\hat{G}_{\mathbf{X}}(\mathbf{t}) = \frac{1}{n} \sum_{j=1}^{n} e^{\langle \mathbf{X}^{(j)}, \ln(\mathbf{t}) \rangle}$$

\subsection*{Poisson PGF (Univariate)}

For a single univariate Poisson random variable $X$ with a fixed mean (rate) $\mu$, the PGF is defined as $G_X(t) = \mathbb{E}[t^X]$.
Using the Poisson probability mass function $P(X=n) = e^{-\mu} \frac{\mu^n}{n!}$, the closed form is:
\begin{align*}
G_X(t) &= \sum_{n=0}^{\infty} P(X=n) t^n = \sum_{n=0}^{\infty} e^{-\mu} \frac{\mu^n}{n!} t^n \\
&= e^{-\mu} \sum_{n=0}^{\infty} \frac{(\mu t)^n}{n!} \\
&= e^{-\mu} e^{\mu t} = e^{\mu(t-1)}
\end{align*}
Recognizing the Taylor series expansion for the exponential function, $e^a = \sum_{n=0}^{\infty} \frac{a^n}{n!}$ where $a = \mu t$, the PGF simplifies to:
$$
G_X(t) = e^{\mu(t-1)} \text{}
$$

\subsection*{Mixed Poisson PGF (Multivariate)}

Let's consider a multivariate observation $\mathbf{X}$ generated by a mixture model, where the conditional distribution of $\mathbf{X}$ given a latent random vector $\mathbf{Z}=\mathbf{z}$ (linked to the rates) is:
$$\mathbf{X}|\mathbf{Z}=\mathbf{z} \sim \bigotimes_{i=1}^{d} \text{Poisson}((\mathbf{A}\mathbf{z})_i)$$
The conditional PGF for the multivariate $\mathbf{X}$ is the product of the independent marginal PGFs, where $\mathbf{A}\mathbf{z}$ is the vector of Poisson rates:
$$
G_{\mathbf{X}|\mathbf{Z}=\mathbf{z}}(\mathbf{t}) = \mathbb{E}\left[\prod_{i=1}^{d} t_i^{X_i} \mid \mathbf{Z}=\mathbf{z}\right] = \prod_{i=1}^d e^{(\mathbf{A}\mathbf{z})_i (t_i - 1)}
$$
This simplifies using vector notation $\langle \cdot, \cdot \rangle$ (and $\mathbf{1}$ as a vector of ones):
$$
G_{\mathbf{X}|\mathbf{Z}=\mathbf{z}}(\mathbf{t}) = e^{\langle \mathbf{A}\mathbf{z}, \mathbf{t}-\mathbf{1}\rangle} \text{}
$$

The mixed Poisson PGF is obtained by taking the expectation of the conditional PGF $G_{\mathbf{X}|\mathbf{Z}=\mathbf{z}}(\mathbf{t})$ over the distribution of the latent mixing variable $\mathbf{Z}$.

Assuming the discrete mixture model defined in the text, where $\mathbf{Z}$ takes one of $K$ fixed values $\mathbf{z}_k$ with probability $\pi_k$:

$$
p(\mathbf{z}) = \sum_{k=1}^{K} \pi_{k}\delta_{\mathbf{z}_{k}}(\mathbf{z})
$$

Then,

$$
G_{\mathbf{X}}(\mathbf{t}) = \mathbb{E}_{\mathbf{Z}}[G_{\mathbf{X}|\mathbf{Z}=\mathbf{Z}}(\mathbf{t})] = \sum_{k=1}^{K} \pi_k G_{\mathbf{X}|\mathbf{Z}=\mathbf{z}_k}(\mathbf{t})
$$
Substituting the conditional PGF yields the closed-form expression for the Mixed Poisson PGF:
$$
G_{\mathbf{X}}(\mathbf{t}) = \sum_{k=1}^{K}\pi_{k}e^{\langle \mathbf{A}\mathbf{z}_{k},\mathbf{t}-\mathbf{1}\rangle} = \sum_{k=1}^{K}\pi_{k}e^{\langle \boldsymbol{\lambda}_{k},\mathbf{t}-\mathbf{1}\rangle}
$$
where $\boldsymbol{\lambda}_k = \mathbf{A}\mathbf{z}_k$ is the vector of Poisson rates for the $k$-th mixture component.

\section{Sampling Strategy}

We sample the Probability-Generating Function (PGF) along complex directions $\mathbf{u}$, scaled by a factor $\Delta$ to ensure numerical stability. We define the sampling points $\mathbf{t} \in \mathbb{C}^d$ as:

$$
\mathbf{t} = \mathbf{1}_d + j \Delta \mathbf{u}
$$

Substituting this into the PGF definition yields the sample vector $\mathbf{y}$:

$$
\mathbf{y} = G_{\mathbf{X}}(\mathbf{t}) = \sum_{k=1}^{K} \pi_k e^{j \Delta \langle \boldsymbol{\lambda}_k , \mathbf{u} \rangle}
$$

Thus, $\mathbf{y}$ represents a sum of damped complex exponentials. Recovering the frequencies $\{\boldsymbol{\lambda}_k\}$ from the observed samples $\{\mathbf{y}\}$ is precisely the spectral estimation problem that the \textsc{Joint ESPRIT} algorithm is designed to solve.