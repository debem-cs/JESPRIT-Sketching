\section{Results}

\subsection{Phase Unwrapping Impact}
In the theoretical derivation of ESPRIT-based methods, phase unwrapping is often cited as a necessary step to resolve the ambiguity of the frequency estimates when the phase arguments exceed the $(-\pi, \pi]$ range. However, we found that applying phase unwrapping in the JESPRIT context actually hurts performance.

Figure \ref{fig:unwrapping_comparison} compares the parameter estimation error with (left) and without (right) phase unwrapping enabled. For this experiment, the mixing matrix $A$ was fixed to a $3 \times 3$ matrix. It can be observed that the unwrapped version consistently yields higher error rates and is robust over a smaller range of grid scale values ($\Delta$).

\subsection{Parameter Sensitivity Analysis}
We evaluate the sensitivity of the JESPRIT algorithm (without phase unwrapping) to its key hyperparameters: the number of directions $M$, the number of snapshots $S$, the number of sample points per line $N$, and the grid scale $\Delta$.

As shown in the subplots of Figure \ref{fig:unwrapping_comparison} (particularly the "Without Phase Unwrapping" panel):
\begin{itemize}
    \item \textbf{Directions ($M$) and Snapshots ($S$):} The estimation error remains stable and low as $M$ and $S$ increase beyond the sufficient lower bounds (related to $d$ and $r$). This suggests that the algorithm is robust to over-sampling in these dimensions, and performance does not degrade with larger values, mainly for $M$.
    \item \textbf{Samples per Line ($N$):} Unlike $M$ and $S$, increasing $N$ excessively can lead to performance degradation. While a certain minimum number of points is required for accuracy, very large $N$ effectively extends the sampling range into regions where the phase arguments may exceed the principal range, causing wrapping issues when $\Delta$ is fixed.
    \item \textbf{Grid Scale ($\Delta$):} This parameter exhibits a distinct "sweet spot." As discussed previously, the error is minimized when $\Delta \approx 1/\max(A)$. Deviating significantly from this value increases the estimation error.
\end{itemize}

These results highlight that while $M$ and $S$ can be chosen generously, $N$ and particularly $\Delta$ require careful tuning to match the signal characteristics.

\subsection{Sample Complexity and Rate Range}
Finally, we analyze the sample complexity of JESPRITâ€”specifically, how many samples are required to successfully recover the latent factors as the problem dimensions grow. We also investigate the impact of the dynamic range of the rates in $A$.

To quantify this, we performed a grid search over varying ambient dimensions $d \in [1, 10]$ and ranks $r \in [1, 10]$. For each $(d, r)$ pair, we conducted 7 independent random trials. A trial was considered successful if the average Mean Relative Error (MRE) for both rates and weights was less than or equal to $10\%$. We tested sample sizes of $N_s \in \{1k, 10k, 50k, 100k\}$.

Simulations were conducted only while the success rate remained above $70\%$. If the success rate dropped below this threshold, further simulations for higher ranks $r$ were halted, as failure was deemed certain. This explains the absence of data points for higher $r$ values in the heatmaps.

We compared the sample complexity for two scenarios:
\begin{enumerate}
    \item \textbf{Small Range}: Rates $A$ drawn from $[0, 100]$.
    \item \textbf{Large Range}: Rates $A$ drawn from $[0, 10000]$.
\end{enumerate}

Figures \ref{fig:sample_complexity_100} and \ref{fig:sample_complexity_10000} show the success rate (percentage of trials that met the $10\%$ error threshold) for each configuration. The results indicate that a larger range of values in $A$ improves the recoverability of the latent factors. With a larger dynamic range, the "directions" in the count space are more distinct, essentially providing a higher effective signal-to-noise ratio for the subspace estimation. This allows the algorithm to correctly discover more latent factors (higher $r$) for a given sample size compared to the small range scenario.

Furthermore, the results suggest that the sample complexity depends primarily on the number of latent factors $r$, rather than the ambient dimension $d$. As observed in the heatmaps, increasing $d$ while keeping $r$ constant results in a negligible increase in the sample size required for successful recovery.

\clearpage

\begin{figure}[p]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=0.45\textheight,keepaspectratio]{imgs/evaluate_parameters_with_unwrapping.png}
        \caption*{With Phase Unwrapping}
    \end{minipage}
    \vfill
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=0.45\textheight,keepaspectratio]{imgs/evaluate_parameters_without_unwrapping.png}
        \caption*{Without Phase Unwrapping}
    \end{minipage}
    \caption{Comparison of estimation error with and without phase unwrapping. The unwrapped version (bottom) shows sensitivity to $\Delta$ but robustness to $M$ and $S$.}
    \label{fig:unwrapping_comparison}
\end{figure}

\clearpage

\begin{figure}[p]
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=0.45\textheight,keepaspectratio]{imgs/sample_complexity_0_to_100.png}
        \caption{Sample Complexity for $A \in {[0, 100]}$.}
        \label{fig:sample_complexity_100}
    \end{minipage}
    \vfill
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=0.45\textheight,keepaspectratio]{imgs/sample_complexity_0_to_10000.png}
        \caption{Sample Complexity for $A \in {[0, 10000]}$.}
        \label{fig:sample_complexity_10000}
    \end{minipage}
    \caption{Comparison of Sample Complexity for different rate ranges. The larger range (bottom) allows for successful recovery of higher ranks.}
\end{figure}
